# ğŸ“Š RAG Benchmark with Arabic Embeddings, FlashRank & Ollama

This project benchmarks **three Arabic embedding models** in a **RAG (Retrieval-Augmented Generation)** pipeline, applied to **Maliki Fiqh datasets**.  
It compares the performance **with and without FlashRank re-ranking**, using **Ollama LLM (llama3:8b)**.

---

## ğŸš€ Features
- âœ… Compare **3 Arabic embeddings** (CAMeLBERT, DistilBERT, AraBERT Large)  
- âœ… Test with **and without FlashRank Arabic reranker**  
- âœ… Dataset: `dataset_700QA.txt` (700 QA in Maliki fiqh)  
- âœ… Evaluation on `qcm_test_140QA.json` (140 MCQ questions with correct answers)  
- âœ… Metrics: **Accuracy, BLEU score, F1-score, Response time**  
- âœ… Results exported to CSV  

---

## ğŸ“‚ Repository Structure
comparaison-RAG-arabe/
â”‚â”€â”€ comparaison_3modeles_embeddings_flasreranking_ollama.py # main script

â”‚â”€â”€ requirements.txt # dependencies

â”‚â”€â”€ README.md # project overview

â”‚â”€â”€ GUIDE.md # detailed usage guide

â”‚â”€â”€ data/

â”‚ â”œâ”€â”€ qcm_test_140QA.json # test QCM file

â”‚ â””â”€â”€ dataset_700QA.txt # knowledge dataset

â”‚â”€â”€ results/

â”‚ â””â”€â”€ rag_comparison_flashrank_arabic.csv # results (generated)
â–¶ï¸ Usage

Make sure Ollama is running locally with the llama3:8b model:

ollama run llama3:8b


Then run the benchmark:

python comparaison_3modeles_embeddings_flasreranking_ollama.py

ğŸ“Š Output
The script performs 6 experiments:

CAMeLBERT + no reranking

CAMeLBERT + FlashRank

DistilBERT + no reranking

DistilBERT + FlashRank

AraBERT Large + no reranking

AraBERT Large + FlashRank

Generated results:

results/rag_comparison_flashrank_arabic.csv

Intermediate summaries in terminal (accuracy, BLEU, F1, response time)

Final aggregated performance table

